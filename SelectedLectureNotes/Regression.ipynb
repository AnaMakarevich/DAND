{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Linear Regression](#linreg)\n",
    "- [Least Squares Algorithm and its Derivation](#LSA)\n",
    "- [Categorical Variables and Linear Regression](#categlinreg)\n",
    "- [Regression: Potential Problems](#problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linreg\"></a>\n",
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Regression - just two varialbles, one which is the explanatory variable (x) and the other one is response variable (y). This type of regression can be easily illustrated with scatterplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Correlation Coefficient (r)</b> - the strength and direction of a linear relationship. $r \\in [-1,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundaries for the strengh of correlation depend on the fiels. General guidelines:  \n",
    "- Strong: $0.7 \\leq |r| < 1.0$   \n",
    "- Moderate: $0.3 \\leq |r| < 0.7$   \n",
    "- Weak: $0.0 \\leq |r| < 0.3$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) } { {\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}} {\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Important:</b> if $r=0$ it DOES NOT necessarily mean that there is no relationship at all. It just means that there is no <b>linear</b> relationships. So, correlation cofficient only captures linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('quizzes-data-1.xlsx')\n",
    "df = df[df.Temp.notnull()&df.Sales.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.95902026],\n",
       "       [0.95902026, 1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df.Temp, df.Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b_0$: The **intercept** is defined as the **predicted value of the response when the x-variable is zero**.\n",
    "\n",
    "$b_i, i \\geq 1$ : The **slope**: for every unit increase in x **the expected change(increase or decrease) in y by the slope, holding all else (other variables) constant**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression equation is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = b_0 + b_1 x_1 + ... + b_n x_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b_0, b_1, ... ,b_n$ are statistic values, whereas $\\beta_0, \\beta_1,..., \\beta_n$ are actual, population parameters. Also, $\\hat{y}$ is predicted value, whereas $y$ is actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"lsa\"></a>\n",
    "# Least Squares Algorithm  and its Derviation\n",
    "Goal: Minimize the sum of the squared vertical distances from the line to points. Objective function will look like this:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E = \\sum_{i=1}^n (y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other loss functions are possible, but this one is the easiest one to work with since it's easy to take it's derivative which is necessary for finding the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our $\\hat{y}^{(i)}$ as:\n",
    "$$(x^{(i)})^T b$$, \n",
    "where $x^{(i)}$ is a vector $[1; {x_{o}^{(i)}}]$ (I'm turning original scalar $x_{o}^{(i)}$ to a vector so that we can pack it into a dot product and then to matrix-vector multiplication. And for the sake of generalisation let's actually make $x^{(i)}$ n-dimensional. I.e. our i-th observation has n features.   \n",
    "By default, all vectors are column vectors. Now, b is a vector $(b_0, b_1, ... b_n)$. Check that we get the same result after these arrangements: \n",
    "\n",
    "$$(x^{(i)})^T b = [1; x_1,...,x_n]^T [b_0, b_1,.. b_n] = b_0 + b_1 x^{(i)}_1 + b_2 x^{(i)}_2 + ... + b_n x^{(i)}_n $$\n",
    "\n",
    "So we can rewrite our objective function as: \n",
    "\n",
    "$$E(b) = \\sum_{i=1}^{n} (y^{(i)} - (x^{(i)})^T b)^2$$ \n",
    "\n",
    "This sum is actually the definiton of a dot product, so we can further rewrite it as: \n",
    "\n",
    "$$E(b) = \\sum_{i=1}^{n} (y^{(i)} - (x^{(i)})^T b)^2 = (y-Xb)^T (y-Xb)$$, \n",
    "\n",
    "where X is a n by 2 matrix with the first column being all 1s. So when we multiply this matrix by vector b, we'll get $\\hat{y}$ vector of predictions. Now we can minimize this function, but first we will expand it: \n",
    "\n",
    "$$E(b) = (y-Xb)^T (y-Xb) = y^T y - y^T X b - b^T X^T y + b^T X^T X b$$\n",
    "\n",
    "Here, it's important to notice that $y^T X b = b^T X^T y$, so we can now write: \n",
    "\n",
    "$$E(b) = y^T y - 2 b^T X^T y + b^T X^T X b $$\n",
    "\n",
    "And now we will take the derivative of this guy and equate it to $\\vec{0}$:  \n",
    "\n",
    "$$\\nabla{E} = - 2 X^T y + 2 X^T X b = \\vec{0} $$\n",
    "\n",
    "And now we can find the b vector as follows: \n",
    "\n",
    "$$X^T y = X^T X b$$\n",
    "\n",
    "$$b = (X^T X)^{-1} X^T y $$  \n",
    "\n",
    "The only possible problem here is that the matrix might appear to be non-invertible and in this case there are special techniques that help to avoid it. Typically, pseudoinverse is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/house_prices.csv')\n",
    "# add ones column\n",
    "df['intercept'] = 1 \n",
    "X = df[['intercept', 'area', 'bathrooms', 'bedrooms']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.dot(np.dot(np.linalg.pinv(np.dot(X.transpose(),X)), X.transpose()),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10072.10704941,   345.91101884,  7345.39171708, -2925.80632748])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 13 Dec 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:11:56</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Thu, 13 Dec 2018   Prob (F-statistic):               0.00\n",
       "Time:                        19:11:56   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = ss.OLS(df['price'], df[['intercept', 'area', 'bathrooms', 'bedrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p-values\n",
    "The p-values are the probabilites of $b_i$ to be 0. This actually shows us the \"usefullness of these parameters. In this case we see that area is a good predictor, while others are not.  \n",
    "\n",
    "**Significant bivariate relationships are not always significant in multiple linear regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared - the amount of variability in the response (y) explained by the model. Closer to 1 - better fit. In fact, R-squared is $r^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"categlinreg\"></a>\n",
    "# Categorical variables in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/0 encoding and Dummy Variables Trap\n",
    "One way to work with categorical variables is to encode them as dummy variables. I.e. create a column for each value of the categoric variable and then encode with 1 or 0 it's presence or absence in the original column. \n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  A\n",
       "1  B\n",
       "2  A\n",
       "3  C\n",
       "4  C"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "values = np.random.choice(['A','B','C'],  size=5)\n",
    "pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  1  0  0\n",
       "1  0  1  0\n",
       "2  1  0  0\n",
       "3  0  0  1\n",
       "4  0  0  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.get_dummies(values)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all looks well and good and we don't see any linear dependencies here. However, we always add the intercept before fitting the regression line, according to the derivation shown in the previous section. So let's look at it now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  intercept\n",
       "0  1  0  0          1\n",
       "1  0  1  0          1\n",
       "2  1  0  0          1\n",
       "3  0  0  1          1\n",
       "4  0  0  1          1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['intercept'] = 1\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is definitely a dependence, because A + B + C = intercept. Which means we will have problems inverting $X^T X$. The problem is that X^T and X will contain linearly dependent columns, too: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 2],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 0, 2, 2],\n",
       "       [2, 1, 2, 5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XtX = np.dot(X.transpose(),X)\n",
    "XtX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that column 4 is actually the sum of columns 1-3. So this is basically a **singular matrix** and we can't invert it, using traditional methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular matrix\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    np.linalg.inv(XtX)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudoinvers (np.linalg.pinv) will still work, but it's just logically incorrect in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The baseline\n",
    "\n",
    "When we drop one column and then run regression, we actually then use this column as a baseline. And the coefficient for the remaining dummy columns allow us to compare with the baseline. Below is the full example of running linear regression with categorical variables. Note that for the column 'neighborhood' we have 3 values: A,B,C, but we use only B and C. A is our baseline. The same for the style: ranch is our baseline, so we only use lodge and victorian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.809</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.809</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4250.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 13 Dec 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:12:04</td>     <th>  Log-Likelihood:    </th> <td> -82944.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.659e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6021</td>      <th>  BIC:               </th> <td>1.659e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-3.833e+05</td> <td>  1.2e+04</td> <td>  -31.995</td> <td> 0.000</td> <td>-4.07e+05</td> <td> -3.6e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>         <td> 5.229e+05</td> <td> 7040.928</td> <td>   74.271</td> <td> 0.000</td> <td> 5.09e+05</td> <td> 5.37e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C</th>         <td>-7168.6285</td> <td> 7639.254</td> <td>   -0.938</td> <td> 0.348</td> <td>-2.21e+04</td> <td> 7807.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lodge</th>     <td> 1.685e+05</td> <td> 9906.629</td> <td>   17.012</td> <td> 0.000</td> <td> 1.49e+05</td> <td> 1.88e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>victorian</th> <td> 7.056e+04</td> <td> 8337.790</td> <td>    8.463</td> <td> 0.000</td> <td> 5.42e+04</td> <td> 8.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 9.996e+04</td> <td> 1.09e+04</td> <td>    9.164</td> <td> 0.000</td> <td> 7.86e+04</td> <td> 1.21e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td> 1.732e+05</td> <td> 7677.152</td> <td>   22.558</td> <td> 0.000</td> <td> 1.58e+05</td> <td> 1.88e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>978.611</td> <th>  Durbin-Watson:     </th> <td>   1.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2926.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.848</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.962</td>  <th>  Cond. No.          </th> <td>    25.9</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.809\n",
       "Model:                            OLS   Adj. R-squared:                  0.809\n",
       "Method:                 Least Squares   F-statistic:                     4250.\n",
       "Date:                Thu, 13 Dec 2018   Prob (F-statistic):               0.00\n",
       "Time:                        19:12:04   Log-Likelihood:                -82944.\n",
       "No. Observations:                6028   AIC:                         1.659e+05\n",
       "Df Residuals:                    6021   BIC:                         1.659e+05\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept  -3.833e+05    1.2e+04    -31.995      0.000   -4.07e+05    -3.6e+05\n",
       "B           5.229e+05   7040.928     74.271      0.000    5.09e+05    5.37e+05\n",
       "C          -7168.6285   7639.254     -0.938      0.348   -2.21e+04    7807.045\n",
       "lodge       1.685e+05   9906.629     17.012      0.000    1.49e+05    1.88e+05\n",
       "victorian   7.056e+04   8337.790      8.463      0.000    5.42e+04    8.69e+04\n",
       "bathrooms   9.996e+04   1.09e+04      9.164      0.000    7.86e+04    1.21e+05\n",
       "bedrooms    1.732e+05   7677.152     22.558      0.000    1.58e+05    1.88e+05\n",
       "==============================================================================\n",
       "Omnibus:                      978.611   Durbin-Watson:                   1.993\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2926.472\n",
       "Skew:                           0.848   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.962   Cond. No.                         25.9\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/house_prices.csv')\n",
    "df_new = df.join(pd.get_dummies(df.neighborhood))\n",
    "df_new = df_new.join(pd.get_dummies(df['style']))\n",
    "df_new['intercept'] = 1\n",
    "lm = ss.OLS(df_new['price'], df_new[['intercept','B','C', 'lodge','victorian', 'bathrooms','bedrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can interpret the baseline. B is more expensive than A, because it has positive coefficient and actually adds about 5e5 to the price. C is, however, cheaper than A because of negative coefficient. The same goes for the styles. Both victorian and lodge styles are more expensive than ranch. \n",
    "Based on the summary, we can say that:\n",
    "- there is statistically significant evidence that the average home price in B differs from the average home price in A (because the p-value is small). \n",
    "- there is no statistically significant evidence that the average home price in C differs from the average home price in A, because the p-value is big \n",
    "- there is statistically significant evidence that the average home price in B differs from the average home price in C because the confidence intervals for them don't overlap.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative -1/0/1 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to ecode categorical variables is with -1/0/1 encoding. This way in the resulting model we will be comparing not to the baseline but to the average, regardless of what column we dropped. In Python there is no automated way to do that - we have to write our own function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"problems\"></a>\n",
    "# Regression: Potential Problems "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Non-linearity of the response-predictor relationships\n",
    "Sometimes the relationships between the variables are far from being linear. In this case the whole model will be useless. We can check for linearity by making a plot of residuals $y-\\hat{y}$ vs x or, in the case of multiple regression, vs $\\hat{y}$. **Ideally**, the plot should show **NO** pattern. \n",
    "\n",
    "If there are curvature patterns in this plot, it suggests that a linear model might not actually fit the data, and some other relationship exists between the predictor variables and response.  \n",
    "\n",
    "Examples of residuals plot: \n",
    "\n",
    "<p><img src=\"residuals.jpg\" width=\"600\" align='center'></p>\n",
    "<i>Source: Udacity</i>\n",
    "\n",
    "If we do identify non-linear relationships, then the simplest approach is to perform transformations of the features - for example, we can use $\\log x$, $\\sqrt{x}$ or $x^2$ instead of x. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlation of error terms\n",
    "\n",
    "When we're using linear regression, we're actually making one important assumption that the errors $\\epsilon_1, ... \\epsilon_n$ are uncorrelated (we can't predict $\\epsilon_{n+1}$ with $\\epsilon_n$. If the errors are correlated, then we get an inadequate picture of the goodness of our model: we get too narrow confidence intervals, too low p-values, etc. The main problem is that we will better predict adjacent event, but the model will be still not as good as it might seem. \n",
    "\n",
    "Correlated error are often the case in time series when the observation are taken at adjacent time points. To check for that, we shoul again **plot the residuals** as a function of time. If the adjacent errors are close to each other, then the errors are correlated. Also, there is a Durbin-Watson test that checks for correlation of error terms at lag 1. \n",
    "\n",
    "ARIMA and ARMA models actually use autocorrelaction for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-constant Variance and Normally Distributed Errors\n",
    "Another assumption we make when creating a regression model is that the variance is constant - $\\text{Var}({\\epsilon_i})=\\sigma^2$. To check for that, we again **plot the residuals**. A sign of non-constant variance is the funnel-shaped plot (plot d.). The problem here is that larger response produces larger error.  Non-constant variace leads to confidence intervals and p-values that are inaccurate.\n",
    "\n",
    "One of the solutions to that is to transform the response variable, Y, by using something like $\\log (Y)$ or $\\sqrt{Y}$. Often, the <a href=\"https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/\">Box-Cox transformation</a> is used.\n",
    "\n",
    "## 4. Outliers/ High leverage points\n",
    "Generally speaking, outlier is a response that lies far away from the regular trends. They can appear as a result of a typo or incorrect recording of information. \n",
    "In order to check for the outliers we again plot the residuals. However, a better way is to plot *studentized* residuals - each residual divided by it's estimated standard error. \n",
    "If studentized value is greater than 3, then it's considered tobe an outlier. \n",
    "\n",
    "**High leverage point** - a data point of predictor featurs that is significantly different from other points. It **significantly** affects the the results and we can get much better fit if we get rid of such points. It can be hard to identify such point in multiple regression case: the features can be normal on their own, but be abnormal when combined. For simle linear regression we can identify high leverage point using the following formula for each point: \n",
    "\n",
    "$$h_i = \\frac{1}{n}+ \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^n (x_i'-\\bar{x})^2}$$\n",
    "\n",
    "High values of $h_i$ indicate high leverage points. \n",
    "\n",
    "## 5. Collinearity\n",
    "\n",
    "Collinear features are features that are closely related to each other. It can cause problems because it will be hard to distinguish the effects of these predictors separately - as a result, we have multiple possible combinations of features for similar values of loss function, while when the featurs are not collinear, the minimum is unique and well defined. With collinear features we have a lot of uncertainty because of that variety of combination for similar values of minimum. Other consequences: \n",
    "- reduces the accuracy of coefficients esitmation \n",
    "- standard error fo $\\beta_i$ increases \n",
    "- t-statistic (defined as $\\frac{\\beta_i}{SE_{\\beta}}$) decreases  \n",
    "- p-values increase \n",
    "- we fail to reject $H_0: \\beta_i = 0$  \n",
    "- we can even get flipped coefficients in the resulting model!!!\n",
    "\n",
    "**Solution:** look at the correlation matrix of the predictors. High values indicate highly correlated pairs. Below is the correlation matrix for the predictors for the houses dataset, described in previous section. The matrix only shows the correlation coefficients bigger than 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>lodge</th>\n",
       "      <th>ranch</th>\n",
       "      <th>victorian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901623</td>\n",
       "      <td>0.891481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.678340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>0.901623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms</th>\n",
       "      <td>0.891481</td>\n",
       "      <td>0.972768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.692416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lodge</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ranch</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>victorian</th>\n",
       "      <td>0.678340</td>\n",
       "      <td>0.705078</td>\n",
       "      <td>0.692416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               area  bedrooms  bathrooms    A    B    C  lodge  ranch  \\\n",
       "area       1.000000  0.901623   0.891481  NaN  NaN  NaN    NaN    NaN   \n",
       "bedrooms   0.901623  1.000000   0.972768  NaN  NaN  NaN    NaN    NaN   \n",
       "bathrooms  0.891481  0.972768   1.000000  NaN  NaN  NaN    NaN    NaN   \n",
       "A               NaN       NaN        NaN  1.0  NaN  NaN    NaN    NaN   \n",
       "B               NaN       NaN        NaN  NaN  1.0  NaN    NaN    NaN   \n",
       "C               NaN       NaN        NaN  NaN  NaN  1.0    NaN    NaN   \n",
       "lodge           NaN       NaN        NaN  NaN  NaN  NaN    1.0    NaN   \n",
       "ranch           NaN       NaN        NaN  NaN  NaN  NaN    NaN    1.0   \n",
       "victorian  0.678340  0.705078   0.692416  NaN  NaN  NaN    NaN    NaN   \n",
       "\n",
       "           victorian  \n",
       "area        0.678340  \n",
       "bedrooms    0.705078  \n",
       "bathrooms   0.692416  \n",
       "A                NaN  \n",
       "B                NaN  \n",
       "C                NaN  \n",
       "lodge            NaN  \n",
       "ranch            NaN  \n",
       "victorian   1.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = df_new[['area','bedrooms','bathrooms','A','B','C','lodge','ranch','victorian']].corr()\n",
    "correlations[correlations>0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem - **multicollinearity**, when collinearity is between more than two variables, which is hard to identify since we can't get a correlation matrix that easily. In this case we calculate **variance inflation factor (VIF)**: \n",
    "\n",
    "$$\\text{VIF}(\\hat{\\beta}) = \\frac{1}{1-R^2_{X_j | X_{-j}}}$$\n",
    "$R^2_{X_j | X_{-j}}$ is the regression of $X_j$ onto all other variables, excluding $X_j$. \n",
    "\n",
    "The minimum value for **VIF** is 1. When VIF = 1, it means there is no collinearity. Anything above 5 (or sometimes 10) signals about collinearity. More on VIF - [here](https://onlinecourses.science.psu.edu/stat501/node/347/)\n",
    "\n",
    "Let's run an experiment on the data we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['intercept','area','bedrooms','bathrooms', 'B','C','lodge','victorian']\n",
    "X = df_new[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif(df: pd.DataFrame, idx: int) -> float:\n",
    "    predictors = df.columns\n",
    "    this_predictor = predictors[idx]\n",
    "    other_predictors = np.append(predictors[0:idx], predictors[idx+1:])\n",
    "    lm = ss.OLS(df_new[this_predictor], df_new[other_predictors]).fit()\n",
    "    return np.round(1/(1-lm.rsquared),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept</td>\n",
       "      <td>17.4840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>area</td>\n",
       "      <td>5.9843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>22.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>19.1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>1.3707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>1.3708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lodge</td>\n",
       "      <td>1.9706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>victorian</td>\n",
       "      <td>2.0465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature      VIF\n",
       "0  intercept  17.4840\n",
       "1       area   5.9843\n",
       "2   bedrooms  22.3125\n",
       "3  bathrooms  19.1940\n",
       "4          B   1.3707\n",
       "5          C   1.3708\n",
       "6      lodge   1.9706\n",
       "7  victorian   2.0465"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vifs = pd.DataFrame({'feature': features, 'VIF': [vif(X, i) for i in range(len(features))]})\n",
    "vifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a method is statsmodels package that calculates VIF. It can be used as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept</td>\n",
       "      <td>17.483993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>area</td>\n",
       "      <td>5.984297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>22.312460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>19.194018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>1.370705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>1.370759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lodge</td>\n",
       "      <td>1.970631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>victorian</td>\n",
       "      <td>2.046512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature        VIF\n",
       "0  intercept  17.483993\n",
       "1       area   5.984297\n",
       "2   bedrooms  22.312460\n",
       "3  bathrooms  19.194018\n",
       "4          B   1.370705\n",
       "5          C   1.370759\n",
       "6      lodge   1.970631\n",
       "7  victorian   2.046512"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vifs2 = pd.DataFrame({'feature': features,'VIF': [variance_inflation_factor(X.values, i) for i in range(len(features))]})\n",
    "vifs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the ready algorithm works a bit differently, probably processing negative R-squared somehow. However, it doesn't influence our interpretation. B,C, lodge and victorian are all \"good\" features, while area, bedrooms and bathroms have high inflation factor and it is reasonable to get rid at least from one of them. Let's see what happens if we remove one variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept</td>\n",
       "      <td>12.023573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>area</td>\n",
       "      <td>5.279540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>6.715363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>1.370560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>1.370757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lodge</td>\n",
       "      <td>1.877402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>victorian</td>\n",
       "      <td>2.020455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature        VIF\n",
       "0  intercept  12.023573\n",
       "1       area   5.279540\n",
       "2  bathrooms   6.715363\n",
       "3          B   1.370560\n",
       "4          C   1.370757\n",
       "5      lodge   1.877402\n",
       "6  victorian   2.020455"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features2 = ['intercept', 'area','bathrooms', 'B','C','lodge','victorian']\n",
    "X2 = df_new[features2]\n",
    "vifs3 = pd.DataFrame({'feature': features2,'VIF': [variance_inflation_factor(X2.values, i) for i in range(X2.shape[1])]})\n",
    "vifs3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the values look reasonable. The value for area is still too big and looks correlated with victorian, but it's still under 5, so we can live with it. \n",
    "\n",
    "**Another solution to multicollinearity** is to actuall combine the values somehow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
