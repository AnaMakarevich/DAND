{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is an example of classification algorithm, when we predict category rather than value. For example, we can use logistic regression to predict **whether a transaction is fraudlent or not** (let's take it as an example for further derivations).\n",
    "\n",
    "It's interesting to note though that linear regression could also be applied when we have just 2 possible values for $Y$ - let's say 1 (fraud) and 0 (not fraud). Using linear regression, we can find the coefficients $\\hat{\\beta}$ and our $\\hat{y}$ then will be showing us the probability of fraud, given the data. In other words: \n",
    "\n",
    "$$\\hat{y} = P(\\text{fraud}|X)$$\n",
    "\n",
    "But this way our predictions CAN be outside of the [0,1] range, and it's hard to interpret it. However, similar approach is used in LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression estimates the probability that Y belongs to some category (in our example - to 1 or 0): \n",
    "\n",
    "$$p(\\text{fraud}=1|X)$$\n",
    "\n",
    "Then we can make prediction, based on our threshold. Typically, it's 0.5. I.e. if $p(\\text{fraud}=1|X) > 0.5$, then predict fraud, else - not fraud. But if we want to be super cautions, we can use different threshold - 0.3, for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability is always in the range of [0,1], so we need to find a suitable function that return values in tha interval. For logistic regression it's a logistic function (aka sigmoid), which looks as follows: \n",
    "\n",
    "$$p(X) = \\frac{e^{Xb}}{1+e^{Xb}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model the **method of maximum likelihood** is used. We can rewrite the above logistic function as follows: \n",
    "\n",
    "$$e^{Xb} = p + p e^{Xb}$$ \n",
    "\n",
    "$$e^{Xb} - p e^{Xb} = p$$\n",
    "\n",
    "$$e^{Xb} (1-p) = p$$\n",
    "\n",
    "$$ \\frac{p}{1-p} = e^{Xb}$$\n",
    "\n",
    "$ \\frac{p}{1-p}$ - the odds of fraud. \n",
    "\n",
    "We can take logarithm of both sides: \n",
    "\n",
    "$$ \\log(\\frac{p}{1-p}) = Xb$$\n",
    "\n",
    "The logarithm of the odds is called **LOGIT** (aka log-odds) - $\\log(\\frac{p}{1-p})$. Logit is linear in X. \n",
    "\n",
    "### Interpretation of the coefficietns \n",
    "\n",
    "In linear regression, one unit increase in $x_i$ meant an increase by $\\beta_i$, in logistic regression one unit increase in $x_i$ means an increase by $\\beta_i$ in log-odds or increase by $e^{\\beta_i}$ in odds. \n",
    "\n",
    "### Maximum Likelihood \n",
    "\n",
    "Idea - we want to find the coefficients $\\hat{b}$ such that that predicted probability is closest to the actual value (i.e. 0 or 1): i.e. closest to 1 for fraudlent transaction, closest to 0 for non-fraudlent transactions.  \n",
    "\n",
    "The likekihood functions looks as follows:  \n",
    "\n",
    "$$l(b)= \\prod_{i:y=1} p(x_i) \\prod_{i':y=1} (1- p(x_i'))$$\n",
    "\n",
    "The goal is to maximize that function with respect to b **OR** minimize $l(b) =  {-\\prod_{i:y=1} p(x_i) \\prod_{i':y=1} (1- p(x_i'))}$ with respect to b. \n",
    "\n",
    "### Null-hypothesis in logistic regression \n",
    "\n",
    "In logistic regression the null-hypothesis is that a coefficient is equal to 0. I.e. it doesn't influence the outcome. Small value tells us that we can reject the null-hypothesis and confirm that the corresponding $b_i$ is not zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.002411\n",
      "         Iterations 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>fraud</td>      <th>  No. Observations:  </th>   <td>  8793</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  8790</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Thu, 20 Dec 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.9633</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>21:55:39</td>     <th>  Log-Likelihood:    </th>  <td> -21.200</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -578.10</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.390e-242</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    9.8709</td> <td>    1.944</td> <td>    5.078</td> <td> 0.000</td> <td>    6.061</td> <td>   13.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday</th>   <td>    2.5465</td> <td>    0.904</td> <td>    2.816</td> <td> 0.005</td> <td>    0.774</td> <td>    4.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration</th>  <td>   -1.4637</td> <td>    0.290</td> <td>   -5.039</td> <td> 0.000</td> <td>   -2.033</td> <td>   -0.894</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.98 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  fraud   No. Observations:                 8793\n",
       "Model:                          Logit   Df Residuals:                     8790\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Thu, 20 Dec 2018   Pseudo R-squ.:                  0.9633\n",
       "Time:                        21:55:39   Log-Likelihood:                -21.200\n",
       "converged:                       True   LL-Null:                       -578.10\n",
       "                                        LLR p-value:                1.390e-242\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      9.8709      1.944      5.078      0.000       6.061      13.681\n",
       "weekday        2.5465      0.904      2.816      0.005       0.774       4.319\n",
       "duration      -1.4637      0.290     -5.039      0.000      -2.033      -0.894\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.98 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.read_csv('data/fraud_dataset.csv')\n",
    "df['intercept'] = 1\n",
    "df['weekday'] = (df.day=='weekday').astype(int)\n",
    "lm = sm.Logit(df.fraud, df[['intercept','weekday','duration']])\n",
    "result = lm.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "\n",
    "**For quantitative variables:**.   \n",
    "For every one unit increase in $x_1$, we expect a multiplicative change in the odds a 1 of a $e^{b_1}$, holding all other variables constant. \n",
    "\n",
    "**For categorical variables:**.   \n",
    "When in category $x_1$, we expect a multiplicative change in the odds of a 1 by$e^{b_1}$ compared to the baseline.\n",
    "\n",
    "**Coefficient < 1**:  \n",
    "It's often convenient to calculate the reciprocal - $\\frac{1}{e^{b_1}}$ and reverse the statement -> to decrease by the obtainted value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "**Accuracy:** $\\frac{\\text{number of correct labels}}{\\text{number of rows}}$  \n",
    "BUT that doesn't work if the categories are too unequal in terms of size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall \n",
    "\n",
    "**Recall**: $$\\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}$$\n",
    "\n",
    "Out of all true positives, what fraction were actually recognized (recalled) as positive? \n",
    "\n",
    "**Precision**: $$\\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$$ \n",
    "\n",
    "Out of all predicted positives, what fraction are really truly positive? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/admissions.csv')\n",
    "df_new = df.join(pd.get_dummies(df.prestige))\n",
    "X = df_new[['gre','gpa',2,3,4]]\n",
    "y = df_new.admit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0)\n",
    "log_mod = LogisticRegression(solver='lbfgs',max_iter=150)\n",
    "log_mod.fit(X_train, y_train)\n",
    "y_pred = log_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56,  0],\n",
       "       [20,  4]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for binary classification has 0,1 column and row names in that order. Also the column are for predicted values, the row - for actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further studies and making notes - <a href=\"https://community.alteryx.com/t5/Data-Science-Blog/ROC-Curves-in-Python-and-R/ba-p/138430\">ROC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
